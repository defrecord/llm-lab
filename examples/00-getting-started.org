#+TITLE: LLM Command Examples
#+PROPERTY: header-args :mkdirp yes :results output :exports both
#+PROPERTY: header-args:sh :dir (concat (projectile-project-root) "data")

* Initial Setup
First, ensure we have a default model set.

#+begin_src shell :llm t :results silent
llm models default llama3.2
#+end_src

* Basic LLM Usage
Let's start with a simple prompt example.

#+begin_src shell :llm t :results file :file haiku.md
llm "Write a haiku about debugging"
#+end_src

#+RESULTS:
[[file:haiku.md]]

* Model Information
View available models and configuration.

#+begin_src shell :llm t :results file :file models.txt
llm models list | head -n 10
#+end_src

#+RESULTS:
[[file:models.txt]]

* Template Management
First, list available templates.

#+begin_src shell :llm t :results file :file templates.txt
llm templates list
#+end_src

#+RESULTS:
[[file:templates.txt]]

Register a basic Python template. Note: We mark this :llm nil as it's a one-time setup.

#+begin_src shell :llm nil
llm --system "Write Python code" --save python-template
#+end_src

#+RESULTS:

* Session Agent Example
Here's an example using the session agent template.

#+begin_src shell :llm t :results file :file session-example.txt
llm -t session-agent "Reviewing LLM commands" 2>&1 | head -n 15
#+end_src

#+RESULTS:
[[file:session-example.txt]]

* Command History and Logs
View recent command history.

#+begin_src shell :llm t :results file :file command-history.txt
llm logs -c | head -n 10
#+end_src

#+RESULTS:
[[file:command-history.txt]]

Analyze model usage.

#+begin_src shell :llm t :results file :file model-usage.txt
llm logs --json | jq -r '.[]|.model' | sort | uniq -c | sort -rn
#+end_src

#+RESULTS:
[[file:model-usage.txt]]

* Note on Interactive Commands
The following commands require user interaction and are not suitable for automated execution:

# Interactive chat session - marked as :llm nil
#+begin_src shell :llm nil
llm chat -m llama3.2
#+end_src

#+RESULTS:

# Git operations - marked as :llm nil
#+begin_src shell :llm nil
git diff --staged | llm -t commit
#+end_src

#+RESULTS:
