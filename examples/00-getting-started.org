#+TITLE: LLM Command Examples
#+SETUPFILE: ./common-headers.org

* Environment Setup
These steps align with the README's Quick Start section.

** 1. Initial Environment
First, ensure the environment is set up:

#+begin_src shell :llm nil :tangle ../data/shell/init.sh
cd .. && make init | tee .../data/init.log | head  | tr -cd '[:print:]\n'
#+end_src

#+RESULTS:
#+begin_example
make[1]: Entering directory '/home/computeruse/.anthropic/sandbox/defrecord/llm-lab'
Running initialization...
Initializing project environment...
[0;32m[0m Created data/sanity
[0;32m[0m Created data/output
[0;32m[0m Created data/sin
[0;32m[0m Created data/images
Installing core dependencies...
Installing LLM providers...
Requirement already satisfied: llm-anthropic in ./.venv/lib/python3.11/site-packages (0.12)
#+end_example


** 2. Environment Check
Verify the environment configuration:

#+begin_src shell :llm nil :tangle ../data/shell/check-env.sh
cd .. && make check-env | tr -cd '[:print:]\n'
#+end_src

#+RESULTS:
#+begin_example
make[1]: Entering directory '/home/computeruse/.anthropic/sandbox/defrecord/llm-lab'
Checking environment...
 ANTHROPIC_API_KEY is set
 GEMINI_API_KEY is set
 GOOGLE_AI_API_KEY is set
 OPENAI_API_KEY is set
 AWS_REGION is set
 GITHUB_TOKEN is set
 AWS_SECRET_ACCESS_KEY is set
 AWS_ACCESS_KEY_ID is set
 UV_SYSTEM_PYTHON is not set - UV System Python
 PYTHONPATH is not set - Python Path
Model configuration:
Using LLM model: gemini-2.0-flash-exp
make[1]: Leaving directory '/home/computeruse/.anthropic/sandbox/defrecord/llm-lab'
#+end_example

** 3. Default Model
Set up the default model:

#+begin_src shell :llm t :results silent  :tangle ../data/llm-model-llama3.2.sh
llm models default llama3.2
#+end_src

* Basic LLM Usage
Let's start with a simple prompt example.

#+begin_src shell :llm t :results both :file ../data/haiku.md :tangle  ../data/haiku.sh
llm "Write a haiku about debugging"
#+end_src

#+RESULTS:
: Gears twist, clues elusive
:  Error's bitter, cold grasp
: Code's stubborn delight

* Model Information
View available models and configuration.

#+begin_src shell :llm t :results both :file ../data/models.txt :tangle ../data/models.sh
llm models list | head -n 5
#+end_src

#+RESULTS:
: OpenAI Chat: gpt-4o (aliases: 4o)
: OpenAI Chat: gpt-4o-mini (aliases: 4o-mini)
: OpenAI Chat: gpt-4o-audio-preview
: OpenAI Chat: gpt-4o-audio-preview-2024-12-17
: OpenAI Chat: gpt-4o-audio-preview-2024-10-01


* Template Management
First, list available templates.

#+begin_src shell :llm t :results both :file ../data/templates.txt :tangle ../data/templates.sh
llm templates list | head
#+end_src

#+RESULTS:
#+begin_example
clojure-function         : system: Write a pure Clojure function following fu...
commit                   : system: name: commit model: llama2 temperature: 0....
dialogue                 : system: ### Dialogue Module Prompt ###  **Backgrou...
elisp-function           : system: Write an Emacs Lisp function following eli...
go-function              : system: Write an idiomatic Go function following G...
js-function              : system: Write a modern JavaScript function using E...
memory                   : system: ### Memory Module Prompt ###  **Background...
org-template             : system: Create org-mode documentation template wit...
python-function          : system: Write a clean, well-documented Python func...
python-numpy             : system: Write a Python function optimized for nume...
#+end_example

Register a basic Python template. Note: We mark this :llm nil as it's a one-time setup.

#+begin_src shell :llm nil :tangle ../data/llm-template-python.sh 
llm --system "Write Python code" --save python-template
#+end_src

#+RESULTS:

* Session Agent Example
Here's an example using the session agent template.

#+begin_src shell :llm t :results file :file ../data/session-example.txt
llm -t session-agent "Reviewing LLM commands" 2>&1 | head -n 15
#+end_src

#+RESULTS:
[[file:../data/session-example.txt]]


* Command History and Logs

** View recent command history

#+begin_src shell :llm t :results file :tangle ../data/command-history.sh :file ../data/command-history.txt
llm logs -n 10
#+end_src

#+RESULTS:
[[file:../data/command-history.txt]]


** Analyze model usage

#+begin_src shell :llm t :results both :file ../data/model-usage.txt :tangle ../data/model-usage.sh
llm logs --json -n 0 | jq -r '.[]|.model' | sort | uniq -c | sort -rn
#+end_src

#+RESULTS:
#+begin_example
     77 llama3.2:latest
     34 deepseek-r1:latest
     10 gemini-2.0-flash-exp
      1 zephyr:latest
      1 session-agent:latest
      1 phi:latest
      1 phi3:latest
      1 mistral:latest
      1 llava:latest
      1 llama3.1:latest
      1 legal-specialist:latest
      1 learnlm-1.5-pro-experimental
      1 jwalsh:latest
      1 jwalsh/technical-writer:latest
      1 jwalsh/staff-engineers:latest
      1 jwalsh/sofia:latest
      1 jwalsh/marcus:latest
      1 jwalsh/jwalsh:latest
      1 jwalsh/jihye:latest
      1 jwalsh/jason:latest
      1 jwalsh/galina:latest
      1 jwalsh/emma:latest
      1 jwalsh/coordinator:latest
      1 jwalsh/alex:latest
      1 gemini-pro
      1 gemini-exp-1206
      1 gemini-2.0-flash-thinking-exp-1219
      1 gemini-2.0-flash-thinking-exp-01-21
      1 gemini-1.5-pro-latest
      1 gemini-1.5-pro-002
      1 gemini-1.5-pro-001
      1 gemini-1.5-flash-latest
      1 gemini-1.5-flash-8b-latest
      1 gemini-1.5-flash-8b-001
      1 gemini-1.5-flash-002
      1 gemini-1.5-flash-001
      1 french-translator:latest
      1 codellama:latest
      1 anthropic/claude-3-opus-20240229
      1 anthropic/claude-3-5-sonnet-20241022
      1 anthropic/claude-3-5-sonnet-20240620
#+end_example


* Note on Interactive Commands
The following commands require user interaction and are not suitable for automated execution:

# Interactive chat session - marked as :llm nil
#+begin_src shell :llm nil
llm chat -m llama3.2
#+end_src

#+RESULTS:
: Chatting with llama3.2:latest
: Type 'exit' or 'quit' to exit
: Type '!multi' to enter multiple lines, then '!end' to finish
: > 

# Git operations - marked as :llm nil
#+begin_src shell :llm nil
git diff --staged | llm -t commit
#+end_src

#+RESULTS:
#+begin_example
import re

def generate_commit_message(diff):
    lines = diff.split('\n')

    # Determine the type of commit based on the first line
    if lines[0].startswith('diff'):
        type = 'feat'
    elif lines[0].startswith('---'):
        type = 'docs'
    elif lines[0].startswith('@@'):
        type = 'refactor'
    elif lines[0].startswith('?'):
        type = 'test'
    elif re.match(r'^fix:.+?$', lines[0]):
        type = 'fix'
    else:
        # If none of the above conditions match, default to 'style' and refactor if diff is not empty
        if lines[0] == '':
            type = 'style'
            if len(lines) > 1:
                type = 'refactor'
        else:
            return None

    scope = ''
    description = ''

    for line in lines[1:]:
        # Parse the commit description
        if line.startswith('++'):
            scope += line[2:]
        elif line == '-':
            continue
        elif line == '+':
            description += line + ' '
        else:
            match = re.match(r'^(\-|--|+)(.*)$', line)
            if match:
                action, action_body = match.groups()
                if action != '-' and action != '+':
                    description += action_body.strip() + ' '

    # Remove any leading or trailing whitespace from the description
    description = description.strip()

    # Check for breaking changes
    if re.search(r'BREAKING CHANGE', description):
        prefix = 'BREAKING CHANGE:'
        description = description.replace('BREAKING CHANGE', '')
    else:
        prefix = ''

    # Format the commit message according to conventional format
    body = f'{prefix}{description}\n\n'

    return f'{type} {scope}: {description}' + body

# Example usage
diff = """
@@ -1,5 + 1,8 @@
- This is a test line.
- This is another test line.

+ This is a new line added by me.
+ This is another new line added by me.
 
@@ -2,3 + 2,4 @@
 This is a test line in the middle.
 This is another test line in the middle.
 This is yet another test line in the middle.
"""

commit_message = generate_commit_message(diff)
print(commit_message)
#+end_example
